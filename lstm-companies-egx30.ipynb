{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8674953,"sourceType":"datasetVersion","datasetId":5199652},{"sourceId":8674960,"sourceType":"datasetVersion","datasetId":5199658},{"sourceId":8675205,"sourceType":"datasetVersion","datasetId":5199835}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install yfinance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tensorflow==2.16.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas_datareader.data import DataReader\nimport yfinance as yf\nfrom pandas_datareader import data as pdr\nyf.pdr_override()\nfrom datetime import datetime, timedelta\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dense, LSTM, GRU, Dropout, Conv1D, MaxPooling1D, Flatten\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"3_PD8RK2x9o1","outputId":"eed3f433-5789-461f-88c3-540c99673dd8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download the data\n# Load and preprocess data\ndata_path = '/kaggle/input/egx30-data1/final.csv'\negx30 = pd.read_csv(data_path)\negx30['Date'] = pd.to_datetime(egx30['Date'])\n\n# Create DataFrame\negx30 = pd.DataFrame(egx30)\n\n# Set 'Date' column as the index\negx30.sort_values('Date', inplace=True)\negx30.set_index('Date', inplace=True)\negx30['Close'] = pd.to_numeric(egx30['Close'].str.replace(',', ''), errors='coerce')\n# Function to convert volume to a uniform unit\ndef convert_volume(vol):\n    if isinstance(vol, str):\n        if 'M' in vol:\n            return float(vol.replace('M', '')) * 1e6\n        elif 'B' in vol:\n            return float(vol.replace('B', '')) * 1e9\n        else:\n            return float(vol)\n    return vol\n\n# Apply the conversion to the 'Vol.' column\negx30['Volume'] = egx30['Volume'].apply(convert_volume)\n\n#'^IXIC' , 'DJI',\n#'AAPL', 'GOOG', 'MSFT', 'AMZN' ,\ntech_list = [ 'AAPL', 'GOOG', 'MSFT', 'AMZN']\nend = datetime.now()\nstart = datetime(end.year - 15, end.month, end.day)\n\ncompany_data = {}\nfor stock in tech_list:\n    company_data[stock] = yf.download(stock, start, end)\ncompany_data['EGX30'] = egx30\n\n# Function to add features like moving averages\ndef add_features(df):\n    df['MA_10'] = df['Close'].rolling(window=10).mean()\n    df['MA_50'] = df['Close'].rolling(window=50).mean()\n    df['MA_200'] = df['Close'].rolling(window=200).mean()\n    df['Volatility'] = df['Close'].rolling(window=10).std()\n    df['Volume'] = df['Volume']\n    df.dropna(inplace=True)\n    return df","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JVv2qpkzyeMK","outputId":"5428e174-fb2c-4d98-fdd1-78ea9a2e658e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Function to create dataset\ndef create_dataset(df, time_step=60):\n    data = df[['Close']].values\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    if len(data) < time_step + 1:\n        return None, None, None  # Not enough data to create the dataset\n    data_scaled = scaler.fit_transform(data)\n    X, Y = [], []\n    for i in range(len(data_scaled) - time_step):\n        X.append(data_scaled[i:i + time_step])\n        Y.append(data_scaled[i + time_step, 0])\n    X, Y = np.array(X), np.array(Y)\n    return X, Y, scaler\n\n# Function to build and train the LSTM model with cross-validation\ndef build_and_train_model(X_train, Y_train, X_val, Y_val, epochs=10, model_type='LSTM'):\n    model = Sequential()\n    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n    if model_type == 'LSTM':\n        model.add(LSTM(100, return_sequences=True))\n    elif model_type == 'GRU':\n        model.add(GRU(100, return_sequences=True))\n    elif model_type == 'Hybrid':\n        model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(50, activation='relu'))\n    model.add(Dropout(0.3))\n    if model_type in ['LSTM', 'GRU']:\n        model.add(LSTM(100, return_sequences=False))\n    model.add(Dropout(0.3))\n    model.add(Dense(50))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    early_stop = EarlyStopping(monitor='val_loss', patience=10)\n    model.fit(X_train, Y_train, epochs=epochs, batch_size=64, validation_data=(X_val, Y_val), callbacks=[early_stop], verbose=2)\n    return model\n\n# Split data into training, validation, and testing sets\ndef split_data(X, Y, train_size=0.8, val_size=0.1, test_size=0.1):\n    assert train_size + val_size + test_size == 1\n    train_split_index = int(len(X) * train_size)\n    val_split_index = int(len(X) * (train_size + val_size))\n    X_train, X_val, X_test = X[:train_split_index], X[train_split_index:val_split_index], X[val_split_index:]\n    Y_train, Y_val, Y_test = Y[:train_split_index], Y[train_split_index:val_split_index], Y[val_split_index:]\n    return X_train, X_val, X_test, Y_train, Y_val, Y_test\n\n# Hyperparameter tuning using grid search\ndef grid_search(X_train, Y_train, X_val, Y_val):\n    best_rmse = float('inf')\n    best_params = {}\n    units= 50\n    dropout = 0.2\n    batch_size = 32\n    model = Sequential()\n    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(Dropout(dropout))\n    model.add(LSTM(units, return_sequences=False))\n    model.add(Dropout(dropout))\n    model.add(Dense(50))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    early_stop = EarlyStopping(monitor='val_loss', patience=10)\n    model.fit(X_train, Y_train, epochs=50, batch_size=batch_size, validation_data=(X_val, Y_val), callbacks=[early_stop], verbose=0)\n    predictions = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(Y_val, predictions))\n\n\n# Cross-validation using K-Fold\ndef k_fold_cross_validation(X, Y, k=5, model_type='LSTM'):\n    kf = KFold(n_splits=k)\n    rmse_scores = []\n    for train_index, val_index in kf.split(X):\n        X_train, X_val = X[train_index], X[val_index]\n        Y_train, Y_val = Y[train_index], Y[val_index]\n        model = build_and_train_model(X_train, Y_train, X_val, Y_val, epochs=100, model_type=model_type)\n        predictions = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(Y_val, predictions))\n        rmse_scores.append(rmse)\n    return np.mean(rmse_scores), np.std(rmse_scores)\n\n# Training separate models for each company\nmodels = {}\nscalers = {}\nfor stock in ['AAPL', 'GOOG', 'MSFT', 'AMZN' , 'EGX30']:\n    df = company_data[stock]\n    X, Y, scaler = create_dataset(df)\n    if X is None or Y is None:\n        print(f\"Not enough data for {stock} after preprocessing.\")\n        continue\n    X_train, X_val, X_test, Y_train, Y_val, Y_test = split_data(X, Y, train_size=0.8, val_size=0.1, test_size=0.1)\n    grid_search(X_train, Y_train, X_val, Y_val)\n    print(f\" {stock}\")\n    model = build_and_train_model(X_train, Y_train, X_val, Y_val, epochs=100, model_type='LSTM')\n    # Save the model with the stock name\n    model.save(f'{stock}_bilstm_model.h5')\n    models[stock] = model\n    scalers[stock] = scaler\n\n\n# Function to predict the next 30 days\ndef predict_next_30_days(stock, last_x_days, time_step=60):\n    model = models[stock]\n    scaler = scalers[stock]\n    last_x_days_scaled = scaler.transform(last_x_days)\n    next_30_days_predicted = []\n\n    for _ in range(30):\n        next_pred = model.predict(last_x_days_scaled.reshape(1, time_step, last_x_days_scaled.shape[1]))\n        next_pred_full = np.concatenate((next_pred, np.zeros((next_pred.shape[0], 5))), axis=1)\n        next_30_days_predicted.append(next_pred[0])\n        last_x_days_scaled = np.append(last_x_days_scaled[1:], next_pred_full, axis=0)\n\n    next_30_days_predicted = scaler.inverse_transform(np.concatenate((np.array(next_30_days_predicted), np.zeros((30, 5))), axis=1))[:, 0]\n    return next_30_days_predicted\n\n# Function to predict and plot results for the last 30 days\ndef predict_and_plot(stock, X_test, Y_test, scaler):\n    model = models[stock]\n    predictions = model.predict(X_test)\n\n    rmse = np.sqrt(mean_squared_error(Y_test, predictions))\n    r2 = r2_score(Y_test, predictions)\n    print(f'{stock} Test RMSE: {rmse}')\n    print(f'{stock} R^2 Score: {r2}')\n\n    predictions = scaler.inverse_transform(np.concatenate((predictions, np.zeros((predictions.shape[0], 5))), axis=1))[:, 0]\n    Y_test = scaler.inverse_transform(np.concatenate((Y_test.reshape(-1, 1), np.zeros((Y_test.shape[0], 5))), axis=1))[:, 0]\n\n    plt.figure(figsize=(14, 5))\n    plt.plot(company_data[stock].index[-len(Y_test):], Y_test, color='green', label='Actual Price')\n    plt.plot(company_data[stock].index[-len(Y_test):], predictions, color='red', label='Predicted Price')\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.title(f'{stock} Price Prediction')\n    plt.legend()\n    plt.show()\n\n# Predict and plot results for the last 30 days for each company\nfor stock in ['AAPL', 'GOOG', 'MSFT', 'AMZN' , 'EGX30']:\n    df = company_data[stock]\n    X, Y, scaler = create_dataset(df)\n    X_train, X_val, X_test, Y_train, Y_val, Y_test = split_data(X, Y, train_size=0.8, val_size=0.1, test_size=0.1)\n    predict_and_plot(stock, X_test, Y_test, scaler)\n\n# Predicting and plotting the next 30 days for all companies\nplt.figure(figsize=(14, 7))\nfor stock in ['EGX30']:\n    last_x_days = company_data[stock][['Close']].values[-60:]\n    next_30_days_prediction = predict_next_30_days(stock, last_x_days)\n    future_dates = [datetime.now() + timedelta(days=i) for i in range(1, 31)]\n    plt.plot(future_dates, next_30_days_prediction, label=f'{stock} Next 30 Days Predictions')\n\nplt.xlabel('Date')\nplt.ylabel('Price')\nplt.title('Next 30 Days Price Prediction for All indices')\nplt.legend()\nplt.show()\n\n# Predicting and plotting the next 30 days for all companies\nplt.figure(figsize=(14, 7))\nfor stock in ['AAPL', 'GOOG', 'MSFT', 'AMZN' ]:\n    last_x_days = company_data[stock][['Close', 'MA_10', 'MA_50', 'MA_200', 'Volatility', 'Volume']].values[-60:]\n    next_30_days_prediction = predict_next_30_days(stock, last_x_days)\n    future_dates = [datetime.now() + timedelta(days=i) for i in range(1, 31)]\n    plt.plot(future_dates, next_30_days_prediction, label=f'{stock} Next 30 Days Predictions')\n\nplt.xlabel('Date')\nplt.ylabel('Price')\nplt.title('Next 30 Days Price Prediction for All Companies')\nplt.legend()\nplt.show()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"dTaausfjyh5z","outputId":"d2282bc0-859f-4ef2-c0d4-45de41669f1f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}